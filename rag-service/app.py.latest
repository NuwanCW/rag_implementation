from flask import Flask, request, jsonify
from pydantic import BaseModel
import logging
import json
import chromadb
from langchain_ollama import ChatOllama
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import SKLearnVectorStore
from langchain_nomic.embeddings import NomicEmbeddings
from langchain.schema import Document
from langchain_core.messages import HumanMessage, SystemMessage
from chromadb.config import Settings
from langchain_community.vectorstores import Chroma
from langchain.llms.base import LLM
from pydantic import Field
from langchain.chains import RetrievalQA

logging.basicConfig(level=logging.INFO)


# Initialize Flask app
app = Flask(__name__)
# Llama model setup
local_llm = "llama3:latest"
llm_json_mode = ChatOllama(model=local_llm, temperature=0, format="json")
llm = ChatOllama(model=local_llm, temperature=0)
chroma_client = chromadb.HttpClient(
    host="chromadb-container",
    port=8000,
    settings=Settings(allow_reset=True, anonymized_telemetry=False),
)
embedding_function = NomicEmbeddings(model="nomic-embed-text-v1.5", inference_mode="local")
vectorstore = Chroma(
    client=chroma_client,  # Use your ChromaDB client
    collection_name="documents",  # Change as needed
    embedding_function=embedding_function,
)
# Logging setup

embedding_model = NomicEmbeddings(model="nomic-embed-text-v1.5", inference_mode="local")
router_instructions = """You are an expert at routing a user question to a vectorstore.
The vectorstore contains documents related to Bible studies and messages.
Use the vectorstore for questions on these topics.
Return JSON with single key, datasource, that is 'vectorstore'."""
# Doc grader instructions
doc_grader_instructions = """You are a grader assessing relevance of a retrieved document to a user question.
If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant."""
# Grader prompt
doc_grader_prompt = """Here is the retrieved document: \n\n {document} \n\n Here is the user question: \n\n {question}. 
This carefully and objectively assess whether the document contains at least some information that is relevant to the question.
Return JSON with single key, binary_score, that is 'yes' or 'no' score to indicate whether the document contains at least some information that is relevant to the question."""
rag_prompt = """You are an assistant for question-answering tasks. 
Here is the context to use to answer the question:
{context} 
Think carefully about the above context. 
Now, review the user question:
{question}
Provide an answer to this questions using only the above context. 
Use three sentences maximum and keep the answer concise.
Answer:"""
hallucination_grader_instructions = """
You are a teacher grading a quiz. 
You will be given FACTS and a STUDENT ANSWER. 
Here is the grade criteria to follow:
(1) Ensure the STUDENT ANSWER is grounded in the FACTS. 
(2) Ensure the STUDENT ANSWER does not contain "hallucinated" information outside the scope of the FACTS.
Score:
A score of yes means that the student's answer meets all of the criteria. This is the highest (best) score. 
A score of no means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.
Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. 
Avoid simply stating the correct answer at the outset."""
# Grader prompt
hallucination_grader_prompt = """FACTS: \n\n {documents} \n\n STUDENT ANSWER: {generation}. 
Return JSON with two two keys, binary_score is 'yes' or 'no' score to indicate whether the STUDENT ANSWER is grounded in the FACTS. And a key, explanation, that contains an explanation of the score."""
# Answer grader instructions
answer_grader_instructions = """You are a teacher grading a quiz. 
You will be given a QUESTION and a STUDENT ANSWER. 
Here is the grade criteria to follow:
(1) The STUDENT ANSWER helps to answer the QUESTION
Score:
A score of yes means that the student's answer meets all of the criteria. This is the highest (best) score. 
The student can receive a score of yes if the answer contains extra information that is not explicitly asked for in the question.
A score of no means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.
Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. 
Avoid simply stating the correct answer at the outset."""
# Grader prompt
answer_grader_prompt = """QUESTION: \n\n {question} \n\n STUDENT ANSWER: {generation}. 
Return JSON with two two keys, binary_score is 'yes' or 'no' score to indicate whether the STUDENT ANSWER meets the criteria. And a key, explanation, that contains an explanation of the score."""
qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=vectorstore.as_retriever(),
    chain_type="refine"
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
# question = "What did he say about Joseph?"
docs = retriever.invoke(question)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vectorstore.as_retriever()
)
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)
docs_txt = format_docs(docs)
rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=question)
generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])
hallucination_grader_prompt_formatted = hallucination_grader_prompt.format(
    documents=docs_txt, generation=generation.content
)
result = llm_json_mode.invoke(
    [SystemMessage(content=hallucination_grader_instructions)]
    + [HumanMessage(content=hallucination_grader_prompt_formatted)]
)
 if json.loads(result.content)['binary_score']=='yes':
    answer = generation.content
answer_grader_prompt_formatted = answer_grader_prompt.format(
    question=question, generation=answer
)
result = llm_json_mode.invoke(
    [SystemMessage(content=answer_grader_instructions)]
    + [HumanMessage(content=answer_grader_prompt_formatted)]
)
    
---------------------
answer = generation.content
answer_grader_prompt_formatted = answer_grader_prompt.format(
    question=question, generation=answer
)
result = llm_json_mode.invoke(
    [SystemMessage(content=answer_grader_instructions)]
    + [HumanMessage(content=answer_grader_prompt_formatted)]
)
json.loads(result.content)
json.loads(result.content)['binary_score']
-------------------
import operator
from typing_extensions import TypedDict
from typing import List, Annotated


class GraphState(TypedDict):
    """
    Graph state is a dictionary that contains information we want to propagate to, and modify in, each graph node.
    """

    question: str  # User question
    generation: str  # LLM generation
    web_search: str  # Binary decision to run web search
    max_retries: int  # Max number of retries for answer generation
    answers: int  # Number of answers generated
    loop_step: Annotated[int, operator.add]
    documents: List[str]  # List of retrieved documents



from langchain.schema import Document
from langgraph.graph import END


### Nodes
def retrieve(state):
    """
    Retrieve documents from vectorstore

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, documents, that contains retrieved documents
    """
    print("---RETRIEVE---")
    question = state["question"]

    # Write retrieved documents to documents key in state
    documents = retriever.invoke(question)
    return {"documents": documents}


def generate(state):
    """
    Generate answer using RAG on retrieved documents

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, generation, that contains LLM generation
    """
    print("---GENERATE---")
    question = state["question"]
    documents = state["documents"]
    loop_step = state.get("loop_step", 0)

    # RAG generation
    docs_txt = format_docs(documents)
    rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=question)
    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])
    return {"generation": generation, "loop_step": loop_step + 1}


def grade_documents(state):
    """
    Determines whether the retrieved documents are relevant to the question
    If any document is not relevant, we will set a flag to run web search

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): Filtered out irrelevant documents and updated web_search state
    """

    print("---CHECK DOCUMENT RELEVANCE TO QUESTION---")
    question = state["question"]
    documents = state["documents"]

    # Score each doc
    filtered_docs = []
    web_search = "No"
    for d in documents:
        doc_grader_prompt_formatted = doc_grader_prompt.format(
            document=d.page_content, question=question
        )
        result = llm_json_mode.invoke(
            [SystemMessage(content=doc_grader_instructions)]
            + [HumanMessage(content=doc_grader_prompt_formatted)]
        )
        grade = json.loads(result.content)["binary_score"]
        # Document relevant
        if grade.lower() == "yes":
            print("---GRADE: DOCUMENT RELEVANT---")
            filtered_docs.append(d)
        # Document not relevant
        else:
            print("---GRADE: DOCUMENT NOT RELEVANT---")
            # We do not include the document in filtered_docs
            # We set a flag to indicate that we want to run web search
            web_search = "no"
            continue
    return {"documents": filtered_docs, "web_search": web_search}


def web_search(state):
    pass
    # """
    # Web search based based on the question

    # Args:
    #     state (dict): The current graph state

    # Returns:
    #     state (dict): Appended web results to documents
    # """

    # print("---WEB SEARCH---")
    # question = state["question"]
    # documents = state.get("documents", [])

    # # Web search
    # docs = web_search_tool.invoke({"query": question})
    # web_results = "\n".join([d["content"] for d in docs])
    # web_results = Document(page_content=web_results)
    # documents.append(web_results)
    # return {"documents": documents}


### Edges


def route_question(state):
    """
    Route question to web search or RAG

    Args:
        state (dict): The current graph state

    Returns:
        str: Next node to call
    """

    print("---ROUTE QUESTION---")
    route_question = llm_json_mode.invoke(
        [SystemMessage(content=router_instructions)]
        + [HumanMessage(content=state["question"])]
    )
    source = json.loads(route_question.content)["datasource"]
    if source == "websearch":
        print("---ROUTE QUESTION TO WEB SEARCH---")
        return "websearch"
    elif source == "vectorstore":
        print("---ROUTE QUESTION TO RAG---")
        return "vectorstore"


def decide_to_generate(state):
    """
    Determines whether to generate an answer, or add web search

    Args:
        state (dict): The current graph state

    Returns:
        str: Binary decision for next node to call
    """

    print("---ASSESS GRADED DOCUMENTS---")
    question = state["question"]
    web_search = state["web_search"]
    filtered_documents = state["documents"]

    if web_search == "Yes":
        # All documents have been filtered check_relevance
        # We will re-generate a new query
        print(
            "---DECISION: NOT ALL DOCUMENTS ARE RELEVANT TO QUESTION, INCLUDE WEB SEARCH---"
        )
        return "websearch"
    else:
        # We have relevant documents, so generate answer
        print("---DECISION: GENERATE---")
        return "generate"


def grade_generation_v_documents_and_question(state):
    """
    Determines whether the generation is grounded in the document and answers question

    Args:
        state (dict): The current graph state

    Returns:
        str: Decision for next node to call
    """

    print("---CHECK HALLUCINATIONS---")
    question = state["question"]
    documents = state["documents"]
    generation = state["generation"]
    max_retries = state.get("max_retries", 3)  # Default to 3 if not provided

    hallucination_grader_prompt_formatted = hallucination_grader_prompt.format(
        documents=format_docs(documents), generation=generation.content
    )
    result = llm_json_mode.invoke(
        [SystemMessage(content=hallucination_grader_instructions)]
        + [HumanMessage(content=hallucination_grader_prompt_formatted)]
    )
    grade = json.loads(result.content)["binary_score"]

    # Check hallucination
    if grade == "yes":
        print("---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---")
        # Check question-answering
        print("---GRADE GENERATION vs QUESTION---")
        # Test using question and generation from above
        answer_grader_prompt_formatted = answer_grader_prompt.format(
            question=question, generation=generation.content
        )
        result = llm_json_mode.invoke(
            [SystemMessage(content=answer_grader_instructions)]
            + [HumanMessage(content=answer_grader_prompt_formatted)]
        )
        grade = json.loads(result.content)["binary_score"]
        if grade == "yes":
            print("---DECISION: GENERATION ADDRESSES QUESTION---")
            return "useful"
        elif state["loop_step"] <= max_retries:
            print("---DECISION: GENERATION DOES NOT ADDRESS QUESTION---")
            return "not useful"
        else:
            print("---DECISION: MAX RETRIES REACHED---")
            return "max retries"
    elif state["loop_step"] <= max_retries:
        print("---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---")
        return "not supported"
    else:
        print("---DECISION: MAX RETRIES REACHED---")
        return "max retries"

# from langgraph.graph import StateGraph
# from IPython.display import Image, display

# workflow = StateGraph(GraphState)

# # Define the nodes
# workflow.add_node("websearch", web_search)  # web search
# workflow.add_node("retrieve", retrieve)  # retrieve
# workflow.add_node("grade_documents", grade_documents)  # grade documents
# workflow.add_node("generate", generate)  # generate

# # Build graph
# workflow.set_conditional_entry_point(
#     route_question,
#     {
#         "websearch": "websearch",
#         "vectorstore": "retrieve",
#     },
# )
# workflow.add_edge("websearch", "generate")
# workflow.add_edge("retrieve", "grade_documents")
# workflow.add_conditional_edges(
#     "grade_documents",
#     decide_to_generate,
#     {
#         "websearch": "websearch",
#         "generate": "generate",
#     },
# )
# workflow.add_conditional_edges(
#     "generate",
#     grade_generation_v_documents_and_question,
#     {
#         "not supported": "generate",
#         "useful": END,
#         "not useful": "websearch",
#         "max retries": END,
#     },
# )

# # Compile
# graph = workflow.compile()
# display(Image(graph.get_graph().draw_mermaid_png()))

# inputs = {"question": "What are the types of agent memory?", "max_retries"
# ===============================


# ==============================
# # Define function to handle grader instructions
# def grade_document_relevance(doc_txt, question):
#     doc_grader_prompt = f"""Here is the retrieved document: \n\n {doc_txt} \n\n Here is the user question: \n\n {question}. 
#     This carefully and objectively assess whether the document contains at least some information that is relevant to the question."""
#     result = llm_json_mode.invoke(
#         [SystemMessage(content=doc_grader_instructions)] + [HumanMessage(content=doc_grader_prompt)]
#     )
#     return json.loads(result.content)

# def grade_hallucination(doc_txt, generation):
#     hallucination_grader_prompt = f"""FACTS: \n\n {doc_txt} \n\n STUDENT ANSWER: {generation}. """
#     result = llm_json_mode.invoke(
#         [SystemMessage(content=hallucination_grader_instructions)] + [HumanMessage(content=hallucination_grader_prompt)]
#     )
#     return json.loads(result.content)

# def grade_answer(question, generation):
#     answer_grader_prompt = f"""QUESTION: \n\n {question} \n\n STUDENT ANSWER: {generation}. """
#     result = llm_json_mode.invoke(
#         [SystemMessage(content=answer_grader_instructions)] + [HumanMessage(content=answer_grader_prompt)]
#     )
#     return json.loads(result.content)

# # Text splitting and vector store setup
# text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
#     chunk_size=1000, chunk_overlap=200
# )

# def create_vector_store(docs_list):
#     doc_splits = text_splitter.split_documents(docs_list)
#     vectorstore = SKLearnVectorStore.from_documents(
#         documents=doc_splits,
#         embedding=NomicEmbeddings(model="nomic-embed-text-v1.5", inference_mode="local"),
#     )
#     return vectorstore.as_retriever(k=3)

# API route for query handling

# ====---------------------
# @app.route("/query", methods=["POST"])
# def query_system():
#     try:
#         data = request.get_json()
#         query_data = QueryRequest(**data)

#         # Retrieve documents using ChromaDB
#         retrieved_docs = retrieve_relevant_documents(query_data.query, query_data.top_k)
#         if not retrieved_docs:
#             return jsonify({"response": "No relevant documents found."}), 200

#         # Split documents and create vector store retriever
#         docs_list = [doc.page_content for doc in retrieved_docs]
#         retriever = create_vector_store(docs_list)

#         # Select top k relevant docs and grade them
#         docs_txt = "\n\n".join([doc.page_content for doc in retrieved_docs])
#         grader_results = [grade_document_relevance(doc.page_content, query_data.query) for doc in retrieved_docs]
#         if all(res['binary_score'] == 'no' for res in grader_results):
#             return jsonify({"response": "No relevant documents found."}), 200

#         # Generate response using LangChain QA chain
#         context = "\n\n".join([doc.page_content for doc in retrieved_docs])
#         rag_prompt = f"""You are an assistant for question-answering tasks. Here is the context to use to answer the question: {context} Now, review the user question: {query_data.query} Provide an answer using only the above context. Use three sentences maximum and keep the answer concise. Answer:"""
#         generation = llm_json_mode.invoke([HumanMessage(content=rag_prompt)])

#         # Grade the hallucinations and final answer
#         hallucination_grade = grade_hallucination(docs_txt, generation.content)
#         answer_grade = grade_answer(query_data.query, generation.content)

#         # Check if everything is okay and return the final response
#         if hallucination_grade['binary_score'] == 'no' or answer_grade['binary_score'] == 'no':
#             return jsonify({"response": "Answer is not appropriate."}), 200

#         return jsonify({"response": generation.content}), 200

#     except Exception as e:
#         logging.error(f"Error processing query: {e}")
#         return jsonify({"error": str(e)}), 500

# if __name__ == "__main__":
#     app.run(host="0.0.0.0", port=9005, debug=True)
